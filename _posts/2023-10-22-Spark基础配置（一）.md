---
layout: post #ensure this one stays like this
read_time: true # calculate and show read time based on number of words
show_date: true # show the date of the post
title:  Spark实践(一)——配置环境
date:   2023-9-3 # XXXX-XX-XX XX:XX:XX XXXX
description: "大数据程序设计实践作业"
img:  posts/20230803/queue.jpg# the path for the hero image, from the image folder (if the image is directly on the image folder, just the filename is needed)
tags: [大数据, Spark, Python]
author: BreejojoDe
github: username/reponame/ # set this to show a github button on the post
toc: yes # leave empty or erase for no table of contents
---

## 在本地虚拟机上配置  
第一个节点采用本地虚拟机，这里已经有配置好的虚拟机，采用本地ssh远程连接。  

这里说一下远程登录的Host配置文件地址，有两个
- "C:\Windows\System32\drivers\etc\hosts"，这个是系统host，但注意，这个命名时不能有"_"这个字符，否则浏览器实测不能替换成ip。

- "C:\Users\17293\.ssh\config"，这个是windows的ssh配置地址，ssh命令host可以用这个，支持直接的私钥地址
  ```
    Host linux_1
	    HostName ***.***.***.*** // ip address
	    User user  // log username
	    IdentityFile "C:\ause\use_for_ssh\id_rsa"  
    ```


### 可以选择更换官方镜像源  

### 允许通过浏览器管理服务器  
这里提供的命令是 `systemctl enable --now cockpit.socket`  

> <br>
> 命令 systemctl enable --now cockpit.socket 用于在 Linux 系统上启用并立即启动 Cockpit Web 控制台的 Socket 服务。Cockpit 是一个用于管理 Linux 服务器的 Web 用户界面。  
> 
> 让我们解释一下这个命令的各部分：
> 
> - systemctl: 这是一个 Linux 系统管理工具，用于启动、停止、启用、禁用和管理系统服务。
> - enable: 这个选项用于启用一个系统服务，使其在系统启动时自动启动。在这个命令中，它用于启用 Cockpit 的 socket 服务，以便系统在启动时启动 Cockpit。
> - --now: 这个选项用于立即启动服务，而不需要等待系统重启。一旦命令执行完毕，Cockpit 就会立即启动。
> - cockpit.socket: 这是 Cockpit Web 控制台的 socket 服务单元的名称。Cockpit 使用 socket 来监听 Web 请求，并与 Web 浏览器建立连接。
> - 执行此命令后，Cockpit 将立即启动，并且系统将配置为在下次启动时自动启动 Cockpit。这意味着你可以立即访问 Cockpit 的 Web 用户界面，而无需等待系统重启。
> 
> 请注意，要使用 Cockpit，你需要在 Web 浏览器中访问服务器的 IP 地址和 Cockpit 的端口（默认情况下为 9090），例如：http://服务器IP地址:9090。你还需要正确配置防火墙以允许流量通过所选的端口。  
> <br>

但首次运行时fail如下：
```
==== AUTHENTICATING FOR org.freedesktop.systemd1.manage-unit-files ===
Authentication is required to manage system service or unit files.
Authenticating as: Da,,, (user)
Password:
==== AUTHENTICATION COMPLETE ===
Failed to enable unit: Unit file cockpit.socket does not exist.
```
系统并没有找到这个服务单元，说明没有安装对应文件。在这个ubuntu镜像里并没有cockpit。  
我们登陆的是user用户，执行安装命令
```
sudo apt-get install cockpit
```

再执行启动，分别启动了`org.freedesktop.systemd1.manage-unit-files`, `org.freedesktop.systemd1.reload-daemon`, `org.freedesktop.systemd1.manage-units`三个服务，输了三次密码，再感叹下linux的权限管理之严格~  
<br>
但是需要访问端口，还没学计网，但按之前的经验，端口似乎是只能被一个服务占据（猜的），所以检查什么端口可用，这里用命令
```
sudo netstat -ap | grep 9090
```
用sudo是为了提权来查看所有进程，返回如下：
```
tcp6       0      0 [::]:9090               [::]:*                  LISTEN      1/init
```
说明没有占用，可以用9090来访问。

这时就可以用 `https://host1:9090/` 这个地址在本地访问虚拟机啦。  

### 所需软件和框架安装
#### 安装Java
教程中使用yum管理，但是卸载了Java11，然后安装Java8，可能是依赖需要，ubuntu中我们重新安装。  
```
dpkg --list | grep openjdk  #查看java包

sudo apt remove openjdk-11-*  #卸载java11

sudo apt install openjdk-8-jdk  #安装java8

sudo apt install openjdk-8-jdk-headless  #安装开发
```

#### 安装Hadoop和Spark

下载地址：
- [Hadoop下载地址](https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz)
- [spark官网地址](https://spark.apache.org/downloads.html)

这里fscp传输采用了WinScp，[官网在这](https://winscp.net/eng/index.php)，这个软件在windows下有较好的文件图形界面（顺便吐槽下大一用Putty包里的pfscp之煎熬，每次都要自己找路径qwq）。直接拖拽就好  
<br>
文件在user目录外传输时，需要用到root权限，这里可以更改配置允许root访问，linux下ssh配置文件位于 `/etc/ssh/sshd_config`，打开后找到：
```
PermitRootLogin prohibit-password
```
权限改为yes即可，root可以ssh访问。但这次还是选择了放在user下。  
<br>
查看文件权限：
```
ls -l
```

解压命令：
```
tar -xvf [file_name]  
    -x 选项表示解压缩操作。
    -v 选项表示在解压缩过程中显示详细信息，也就是 verbose 模式。
    -f 选项后面跟着要解压的 Tar 存档文件的名称。
```

解压时突然出现空间不足，原本分的20GB的空间安了好多库之后直接塞满......  
又花了好久扩容虚拟机空间，简要记录下.  
```
df -T
df -h [/path]  #列出系统上所有的文件系统，包括它们的大小、已用空间、可用空间和挂载点。
lsblk -l  #查看是否识别空间
sudo apt install gparted  #安装gparted，linux下动态扩容, 在图形界面操作
```

移动文件
```
mv [path1] [path2]
```

解压后要把文件分别放在Hadoop和Spark两个文件夹中，`mkdir [dir1] [dir2] [dir3]` 可以创建多个文件夹。  
在 `/Hadoop`中，需要创建数据存放的文件夹:	tmp、hdfs、hdfs/data、hdfs/name。  
```
mkdir tmp hdfs hdfs/data hdfs/name
```

#### 安装python
这个大部分linux自带，可以更新，注意查看版本
```
python3 --version
```
此外还要单独安装pip3，使用命令：
```
sudo apt install python3-pip
```
这时python3运行组件安装完毕

#### 安装 Jupyter Notebook
这是一个可以提供在局域网内跨机编写python交互式界面的应用  ，在linux下安装
```
pip3 install jupyter
```
当然这时默认源下载，但因为网络问题经常出现失败现象，满屏幕飘红，要么 `time out` ，要么` WARNING: Retrying after connection broken`   
当然，我们还有一个合情合理的方法下载，就是借用一下“东方小哈工”清华大学的源来下载。（啥时候窝工也能有个这样的源 qwq ）
```
pip3 install jupyter -i https://pypi.tuna.tsinghua.edu.cn/simple
```
教程中给我们设置了默认源为阿里源的方法
```
pip config set global.index-url https://mirrors.aliyun.com/pypi/simple
```
<br>

这里我们再设置一下Jupyter的拓展，方便我们使用  
```
pip3 install jupyter_nbextensions_configurator -i https://pypi.tuna.tsinghua.edu.cn/simple jupyter  #清华源
pip3 install jupyter_contrib_nbextensions -i https://pypi.tuna.tsinghua.edu.cn/simple jupyter  #清华源
```

安装后有警告说PATH没有添加这个执行路径
```
WARNING: The script jupyter-contrib is installed in '/home/user/.local/bin' which is not on PATH.
Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
```

在网络上查询到的解决方法是修改配置
```
nano ~/.bashrc

在行末添加：
export PATH="$PATH:$HOME/.local/bin"

保存后再执行生效命令更新配置应用：
source ~/.bashrc
```
<br>

这时再继续调用jupyter来启动配置
```
jupyter contrib nbextension install --user
jupyter nbextensions_configurator enable --user
```
但是，但是！发现jupyter这个逆子居然报错了！！！  
```
ModuleNotFoundError: No module named 'notebook.base'
``````
找不到这个包，这时检查jupyter里有没有这个包  `jupyter --version`  
发现有，这时可能是版本问题，遂百度。  
查到要低于7.0版本的notebook，而安装的是7.0.3  

直接强行降级
```
pip3 install -U "notebook<6.9" -i https://pypi.tuna.tsinghua.edu.cn/simple
```

然后就重新启动jupyter配置

### 配置文件

#### 配置jupyter notebook
为jupyter notebook新建工作目录,在Spark的安装目录下：
```
mkdir source
mkdir source/pyspark
```

执行命令：
```
jupyter notebook --generate-config
```
这个命令会生成Jupyter Notebook的配置文件，通常保存在用户目录下的.jupyter文件夹中。
`Writing default config to: /root/.jupyter/jupyter_notebook_config.py`  

打开Python界面，运行下述代码，产生web访问密码：
```
from notebook.auth import passwd
passwd()
```
这个步骤是为Jupyter Notebook 设置密码。运行代码后，会要求输入一个密码，可以输入回车，将会生成一个密码散列字符串，复制这个字符串以备后用。  

下一步生成jupyter notebook的配置文件
```
touch /root/.jupyter/jupyter_notebook_config.py
```
然后打开这个文件，修改以下配置,左侧不要留空格：  
- c.NotebookApp.allow_root = True：允许Jupyter在root用户下运行。
- c.NotebookApp.ip = '0.0.0.0'：允许Jupyter监听所有可用的IP地址。
- c.NotebookApp.notebook_dir = '/Spark/source'：指定Jupyter的工作目录为/Spark/source，这是你之前创建的目录。
- c.NotebookApp.open_browser = False：禁用自动打开浏览器。
- c.NotebookApp.password = '上面生成的密码加密字符串'：将上一步生成的密码散列字符串填入这个配置项中。  

配置好以后，在命令行启动Jupyter Notebook 服务器，运行：
```
jupyter notebook
```
这时就可以进入jupyter notebook网页编辑界面了

#### 编辑/etc/hosts文件
这个格式同windows下
```
***.***.***.*** [host1_name]
***.***.***.*** [host2_name]
```

#### 编辑/etc/profile
这里有给出的模板，但是我们要关注自己的版本号，模板如下：
```
export JAVA_HOME=/etc/alternatives/java_sdk
export HADOOP_HOME=/Hadoop/hadoop-3.3.4
export SPARK_HOME=/Spark/spark-3.3.2-bin-hadoop3
export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export JAVA_LIBRARY_PATH=$HADOOP_HOME/lib/native
export LD_LIBRARY_PATH=$JAVA_LIBRARY_PATH
export PATH=$PATH:$JAVA_HOME/jre/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$SPARK_HOME/bin
export PYSPARK_DRIVER_PYTHON=jupyter
export PYSPARK_DRIVER_PYTHON_OPTS=notebook
```
但注意，变量需要是自己的，这里的HADOOP_HOME和SPARK_HOME与之前自己的设置相同。JAVA_HOME可以用下面的命令追踪：
```
readlink -f `which java`
```
编辑好后输入到/etc/profile内。更新配置。
```
source /etc/profile
```

#### 配置Hadoop
1. 配置/Hadoop/hadoop-x.x.x/etc/hadoop目录下的core-site.xml
   ```xml
   <configuration>
	<property>
        <name>fs.defaultFS</name>
        <value>hdfs://Host1:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>file:/Hadoop/tmp</value>
    </property>
    <property>
        <name>io.file.buffer.size</name>
        <value>131072</value>
    </property>
   </configuration>
   ```
2. /Hadoop/hadoop-x.x.x/etc/hadoop/hdfs-site.xml
   ``` xml
   <configuration>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:/Hadoop/hdfs/name</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:/Hadoop/hdfs/data</value>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>Host1:9001</value>
    </property>
    <property>
    		<name>dfs.webhdfs.enabled</name>
    		<value>true</value>
    </property>
    <property>
    		<name>dfs.blocksize</name>
    		<value>16777216</value>
    </property>
    <property>
    		<name>dfs.namenode.handler.count</name>
    		<value>100</value>
    </property>
   </configuration>
   ```

3. 在脚本etc/hadoop/hadoop-env.sh中添加：
   ```
   export	JAVA_HOME=/etc/alternatives/java_sdk
   ```

4. 在文件etc/hadoop/workers 写入：（去掉localhost）
   ```
   Host1
   Host2
   ```


#### 配置Spark

/Spark/spark-x.x.x-bin-hadoopx.x/conf/spark-env.sh  
```
export SPARK_MASTER_HOST Host1 #主机名称
```  

conf/workers
```
# 添入计算单元
Host1
Host2
```
不得不说Spark配置确实比Hadoop简单很多


### SSH密钥登录
这步操作是在主机上进行的，用主节点访问各个机器。  

这里先搜索一下之前用的.ssh文件在哪里，user执行命令不太方便，先改在root下操作：
```
su
```
再查找文件
```
find / -type d -name .ssh
```
<br>

似乎用root来ssh连接会简化点过程，避免用user暴雷，我们为root配置密钥。  

首先更改全局ssh配置，位于 `/etc/ssh/sshd_config`
```
PubkeyAuthentication yes  #将这项改为yes并取消注释
```
这一步每个连接的机器都要进行，便于ssh连接
<br>

接下来生成并分发密匙，在根目录下生成密匙，会储存到.ssh中：
```
ssh-keygen -t rsa -b 2048
```
在.ssh下生成两个文件，一个公钥一个私钥。  
.ssh下需要一个文件 `authorized_keys`，用于储存其他计算机生成的公钥，储存格式是每行一个公钥，不能有其他多余字符。拥有私钥的计算机可以ssh登录拥有对应公钥的计算机。  
所以在本系统中，将所有机器的公钥储存在一个认证文件里并分发给所有计算机（储存在/root/.ssh/下）。


<br>

## 在云服务器上配置
**这里只记录与本地有明显不同的地方**

### 开放端口9090
需要自己配置安全组规则，开放9090

### 注意ip地址，在云服务器服务商界面获得
在控制台ifconfig获得的是服务器被分配的本地ip，无法公网访问，ssh无法连接


## 进行工作配置
注意hadoop和Spark的路径是否添加到环境变量

### 初始化并启动Hadoop
```
bin/hdfs namenode -format
```
这个是位于hadoop安装目录下的bin，用于初始化namenode节点（在namenode机器上）。  
注意实际操作不要多次使用，每次格式化会清空NameNode再初始化新的。  
<br>

在start-dfs.sh和stop-dfs.sh文件代码开始处，添加下列参数
```
HDFS_DATANODE_USER=root
HDFS_DATANODE_SECURE_USER=root
HDFS_NAMENODE_USER=root
HDFS_SECONDARYNAMENODE_USER=root
```
这是添加DataNode，NameNode和备用NameNode节点的用户，不过这里都是root。  
通常情况下，建议为每个 Hadoop 组件配置一个专用的非特权用户，并确保该用户具有所需的权限来访问 Hadoop 文件系统和其他资源。这有助于降低潜在的风险，并提高系统的安全性。

这时可以启动`sbin/start-dfs.sh`来开启Hadoop集群了，如果在前面**编辑/etc/profile**的步骤正确，则可以直接执行脚本
```
start-dfs.sh  #开启集群

stop-dfs.sh  #关闭集群
```
在这一步会显现前期很多准备工作没做好的错误，比如**ssh配置错误导致拒绝联机、ip地址错误没有打开、JAVA_HOME路径错误导致找不到文件......等等**，需要逐一排查。而start-dfs.sh成功启动就说明Hadoop集群已经构建成功一半了。  

下一步进行集群的初始化，首先检查hadoop是否能运行
```
hadoop version
```
<br>

这里需要进行hadoop fs命令的介绍
```
hadoop fs
# 这是一个用于与Hadoop分布式文件系统（HDFS）进行交互的命令。你可以使用它来执行各种文件和目录操作，如创建、删除、复制、移动文件和目录等。
```

下面是简单的fs命令介绍：
```
#创建目录：用于在HDFS中创建目录。
hadoop fs -mkdir /path/to/directory

#上传文件：用于将本地文件上传到HDFS。
hadoop fs -put /local/path/to/file /hdfs/path/to/destination

#下载文件：用于将HDFS中的文件下载到本地文件系统。
hadoop fs -get /hdfs/path/to/file /local/path/to/destination

#复制文件：用于在HDFS中复制文件。
hadoop fs -cp /source/path /destination/path

#移动文件：用于在HDFS中移动文件或重命名文件。
hadoop fs -mv /source/path /destination/path

#删除文件或目录：用于从HDFS中删除文件或目录。
hadoop fs -rm /path/to/file
hadoop fs -rmdir /path/to/directory

#列出文件和目录：用于列出HDFS中的文件和目录。
hadoop fs -ls /path

#查看文件内容：用于查看HDFS中文本文件的内容。
hadoop fs -cat /path/to/file

#更改文件或目录权限：用于更改HDFS中文件或目录的权限。
hadoop fs -chmod <权限> /path/to/file_or_directory

#查找文件：用于在HDFS中查找文件。
hadoop fs -find /path/to/search -name "filename"

#统计目录大小：用于查看HDFS中目录的大小。
hadoop fs -du -s -h /path/to/directory

#可以运行hadoop fs命令时使用-help标志。
```
<br>

这里我们按文件给的配置：
```
hadoop fs -mkdir /sparkdata
```
但在尝试时却返回错误
```
mkdir: Call From Breejojo1/127.0.1.1 to Host1:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
```
这是NameNode的功能没有执行，首先检查namenode是否运行，jps命令用于列出当前正在运行的Java进程，并显示它们的进程ID（PID）以及它们的主要角色。通常，Hadoop集群中的不同角色（如**NameNode、DataNode、SecondaryNameNode等**）都在不同的Java进程中运行：
```
jps
```
```
返回如下：
8970 SecondaryNameNode
9115 Jps
8783 DataNode
```
显然，我们并没有看到NameNode进程，这是一个关键的组件，负责管理 Hadoop 集群的元数据。由于缺少 NameNode 进程，导致 Hadoop 集群在端口 9000 上无法监听连接请求。  

这里并没有查看NameNode的日志，而是选择了一个懒蛋方法：关闭Hadoop集群后重新格式化本地NameNode，然后再启动试试
```
stop-dfs.sh

bin/hdfs namenode -format

start-dfs.sh

jps
```
这时进程里已经有了NameNode，运气很好，问题解决了，这时来建立文件夹并传输文件：
```
hadoop fs -mkdir /sparkdata

hadoop fs -put
```
<br>

然后更改下文件权限，使其可以在网络上读写
```
hadoop fs -chmod -R 777 /sparkdata  #777权限过广，实际操作不建议使用
```
- -R: 这是一个选项，表示递归地应用权限更改。如果目录中包含子目录和文件，递归选项将确保所有子目录和文件都具有相同的权限。


#### Web查看Hadoop状态
浏览HDFS，在浏览器输入：
```
http://Host1:9870/  # 必须启动hadoop集群才能用
```
其中，9870是HDFS的namenode使用的默认端口，这个可以在hadoop的 `etc/hadoop/hdfs-site.xml`里更改。

访问前要保证9870端口是对外开放的，在云服务器要注意安全组规则。


### 启动Spark
启动Spark，脚本目录在 `/Spark/spark-3.2.0-bin-hadoop3.2/sbin/start-all.sh`
```
start-all.sh
```
但返回中有报错信息：
```
Starting resourcemanager
ERROR: Attempting to operate on yarn resourcemanager as root
ERROR: but there is no YARN_RESOURCEMANAGER_USER defined. Aborting operation.
Starting nodemanagers
ERROR: Attempting to operate on yarn nodemanager as root
ERROR: but there is no YARN_NODEMANAGER_USER defined. Aborting operation.
```
<br>

检查了.......好蠢的错误，spark顶目录下的./conf/spark-env.sh和./conf/workers名字都写错了......
<br>

连接后可以登录Web UI查看master监控界面：
```
浏览器：http://host1:8080/
```
<br>

但是监控界面发现worker只有一个，并没有服务器，多次检查都是这样。  
思来想去，可能又是服务器的端口没有开放，在网上查了一下Spark的master和worker和master通信端口，似乎7078和7077端口都有使用，[参考链接1](https://blog.csdn.net/qq_45287265/article/details/108672251)，[参考连接2](https://www.cnblogs.com/zhipeng-wang/p/14043454.html)

```
1）Spark历史服务器端口号：18080 （类比于Hadoop历史服务器端口号：19888）

2）Spark Master Web端口号：8080（类比于Hadoop的NameNode Web端口号：9870(50070)）

3）Spark Master内部通信服务端口号：7077 （类比于Hadoop（高版本）的8020(9000)端口）

4）Spark查看当前Spark-shell运行任务情况端口号：4040

5）Hadoop YARN任务运行情况查看端口号：8088
```

在本地master上检查了一下：
```
root@Breejojo1:/home/user/HIT/Spark/spark-3.4.1-bin-hadoop3# sudo netstat -ap | grep 7078
unix  3      [ ]         STREAM     CONNECTED     37078    3373/python3
root@Breejojo1:/home/user/HIT/Spark/spark-3.4.1-bin-hadoop3# sudo netstat -ap | grep 7077
tcp6       0      0 Breejojo1:7077          [::]:*                  LISTEN      18354/java
tcp6       0      0 localhost:47428         Breejojo1:7077          ESTABLISHED 18517/java
tcp6       0      0 Breejojo1:7077          localhost:47428         ESTABLISHED 18354/java
```












1. 连接Spark集群的master主机，在Spark安装目录下，运行命令（注意目录的相对位置要正确）：
/Spark/spark-3.3.2-bin-hadoop3/bin/pyspark --master spark://Host1:7077  

1. 可以通过浏览器访问Jupyter Notebook，编辑运行代码，地址是：
http://Host1:8888/  

1. 8081端口是每个节点的worker
http://host1:4040  查看job信息



## Spark和Hadoop检测不到Datanode/Worker/Slave
查看可能是地址回环问题，不要传输local
